# ğŸ“„ HR Policy Assistant

A production-ready RAG (Retrieval-Augmented Generation) application that lets you upload an HR/Employee Handbook PDF and ask natural language questions about company policies.

Built with **FastAPI**, **Streamlit**, **LangChain**, **OpenAI**, and **Pinecone**.

---

##  Project Structure

```
hr-policy-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py              # FastAPI app factory & middleware
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ health.py       # GET /health
â”‚   â”‚       â”œâ”€â”€ upload.py       # POST /upload
â”‚   â”‚       â””â”€â”€ ask.py          # POST /ask
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py           # Settings (env vars, defaults)
â”‚   â”‚   â””â”€â”€ session_store.py    # In-memory session management
â”‚   â”‚
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ embeddings.py       # Embedding model setup
â”‚       â”œâ”€â”€ indexer.py          # PDF loading & chunking
â”‚       â”œâ”€â”€ retriever.py        # Vector store retriever
â”‚       â””â”€â”€ rag_chain.py        # Prompt + LLM + chain
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                  # Streamlit UI
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts.py              # Centralized prompt templates
â”‚
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ main.py                     # Backend entry point
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Clone & install dependencies

```bash
git clone <repo-url>
cd hr-policy-assistant
pip install -r requirements.txt
```

### 2. Configure environment variables

```bash
cp .env.example .env
# Edit .env with your actual API keys
```

### 3. Run the backend

```bash
python main.py
# API available at http://localhost:8000
# Swagger docs at http://localhost:8000/docs
```

### 4. Run the frontend (new terminal)

```bash
streamlit run frontend/app.py
# UI available at http://localhost:8501
```

---

## ğŸ”‘ Environment Variables

| Variable | Description | Required |
|---|---|---|
| `OPENAI_API_KEY` | Your OpenAI API key | Yes |
| `PINECONE_API_KEY` | Your Pinecone API key | Yes |
| `PINECONE_INDEX_NAME` | Name of your Pinecone index | Yes |
| `API_BASE_URL` | Backend URL for the frontend | No (default: `http://localhost:8000`) |
| `API_HOST` | Host to bind the backend server | No (default: `0.0.0.0`) |
| `API_PORT` | Port for the backend server | No (default: `8000`) |

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/health` | Health check |
| `POST` | `/upload` | Upload & index a PDF |
| `POST` | `/ask` | Ask a question about the document |

### POST `/upload`
Accepts `multipart/form-data`:
- `file` â€” PDF file
- `openai_key` â€” OpenAI API key
- `pinecone_key` â€” Pinecone API key
- `index_name` â€” Pinecone index name

### POST `/ask`
Accepts JSON:
```json
{
  "session_id": "uuid",
  "question": "How many vacation days do I get?",
  "openai_key": "sk-..."
}
```

---

## ğŸ§± Architecture

```
PDF Upload
    â”‚
    â–¼
PyPDFLoader â†’ RecursiveTextSplitter â†’ OpenAI Embeddings â†’ Pinecone VectorStore
                                                                    â”‚
User Question                                                       â”‚
    â”‚                                                               â–¼
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º MMR Retriever â†’ Top-K Chunks
                                                                    â”‚
                                                                    â–¼
                                                          ChatPromptTemplate
                                                                    â”‚
                                                                    â–¼
                                                          GPT-4o-mini â†’ Answer
```

---

## âš™ï¸ Configuration Tuning

All chunking, retrieval, and model settings live in `backend/core/config.py`. Key parameters:

- `CHUNK_SIZE` â€” Token size per chunk (default: 250)
- `CHUNK_OVERLAP` â€” Overlap between chunks (default: 30)
- `RETRIEVER_K` â€” Number of chunks returned (default: 3)
- `RETRIEVER_FETCH_K` â€” Candidates before MMR reranking (default: 15)
- `LLM_MODEL` â€” OpenAI model name (default: `gpt-4o-mini`)
- `LLM_MAX_TOKENS` â€” Max tokens in response (default: 120)

---

## ğŸ“¦ Tech Stack

- **FastAPI** â€” REST API backend
- **Streamlit** â€” Interactive frontend
- **LangChain** â€” RAG orchestration
- **OpenAI** â€” Embeddings (`text-embedding-3-small`) + LLM (`gpt-4o-mini`)
- **Pinecone** â€” Vector database
- **PyPDF** â€” PDF parsing
